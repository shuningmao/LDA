{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reference: \n",
    "\n",
    "orignal paper: https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=12&cad=rja&uact=8&ved=2ahUKEwjzx5aL2rfkAhUPjq0KHe51AV0QFjALegQIARAC&url=http%3A%2F%2Fwww.jmlr.org%2Fpapers%2Fvolume3%2Fblei03a%2Fblei03a.pdf&usg=AOvVaw1xRN0-HZ0xS5mhxzNFmHhF\n",
    "\n",
    "reference1: https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24\n",
    "\n",
    "reference2: https://towardsdatascience.com/nlp-extracting-the-main-topics-from-your-dataset-using-lda-in-minutes-21486f5aa925\n",
    "\n",
    "gensim package manual: https://radimrehurek.com/gensim/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "choose data from sklearn.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "logging.basicConfig() # to show logging message of what is happening behind the scene.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sci.med', 'sci.space']\n",
      "(1187,) (790,)\n"
     ]
    }
   ],
   "source": [
    "categories = ['sci.med', 'sci.space']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', shuffle = True, categories = categories)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', shuffle = True, categories = categories)\n",
    "\n",
    "print(list(newsgroups_train.target_names))\n",
    "\n",
    "print(newsgroups_train.filenames.shape, newsgroups_test.filenames.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Pre-processing  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each document, step 1. tokenize (simple_preprocess) step 2. filter out stopwords and the short words (STOPWORDS, len(token)>3) step 3. transform to the word stem (lemmatize_stemming)\n",
    "\n",
    "E.g. \n",
    "sentence: \"This is a sentence for illustration of the data pre-processing.\"\n",
    "\n",
    "    After step 1: ['this', 'is', 'sentence', 'for', 'illustration', 'of', 'the', 'data', 'pre', 'processing']\n",
    "\n",
    "    After step 2: ['sentence', 'illustration', 'data', 'processing']\n",
    "\n",
    "    After step 3: ['sentenc', 'illustr', 'data', 'process']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Loading Gensim and nltk libraries\n",
    "'''\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "\n",
    "from gensim.utils import lemmatize\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Write a function to perform the pre processing steps on the entire dataset\n",
    "'''\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "# Tokenize and lemmatize\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in simple_preprocess(text) :\n",
    "        if token not in STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "            \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = []\n",
    "\n",
    "for doc in newsgroups_train.data:\n",
    "    processed_docs.append(preprocess(doc))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Build a Dictionary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Save each word/stem into a dictionary (Dictionary)\n",
    "2. Filter out the words with extreme occurrence (filter_extremes)\n",
    "3. Turn the words into bow_corpus using the dictionary in step 1 and documents (doc2bow). With bow, each word is matched with the frequency of occurrence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create a dictionary from 'processed_docs' containing the number of times a word appears \n",
    "in the training set using gensim.corpora.Dictionary and call it 'dictionary'\n",
    "'''\n",
    "dictionary = Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "OPTIONAL STEP\n",
    "Remove very rare and very common words:\n",
    "\n",
    "- words appearing less than 15 times\n",
    "- words appearing in more than 10% of all documents\n",
    "'''\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.1, keep_n= 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create the Bag-of-words model for each document i.e for each document we create a dictionary reporting how many\n",
    "words and how many times those words appear. Save this to 'bow_corpus'\n",
    "'''\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: LDA Model with Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Train LDA model with BOW (LdaMulticore), this uses multicore in computer for faster speed.\n",
    "2. Print out the topics.\n",
    "3. Save model to path (cPickle.dump)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaMulticore\n",
    "import _pickle as cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Train your lda model using gensim.models.LdaMulticore and save it to 'lda_model_bow'\n",
    "'''\n",
    "lda_model_bow =  LdaMulticore(bow_corpus, \n",
    "                                   num_topics = 10, \n",
    "                                   id2word = dictionary,                                    \n",
    "                                   passes = 10,\n",
    "                                   workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5,\n",
       "  '0.010*\"rocket\" + 0.009*\"model\" + 0.008*\"theori\" + 0.008*\"scientif\" + 0.007*\"henri\"'),\n",
       " (4,\n",
       "  '0.020*\"health\" + 0.012*\"softwar\" + 0.012*\"report\" + 0.011*\"henri\" + 0.010*\"toronto\"'),\n",
       " (8,\n",
       "  '0.018*\"fred\" + 0.017*\"dseg\" + 0.016*\"higgin\" + 0.014*\"digex\" + 0.014*\"mccall\"'),\n",
       " (0,\n",
       "  '0.035*\"alaska\" + 0.021*\"aurora\" + 0.018*\"nsmca\" + 0.012*\"acad\" + 0.012*\"astronaut\"'),\n",
       " (7,\n",
       "  '0.031*\"food\" + 0.014*\"water\" + 0.010*\"sensit\" + 0.009*\"studi\" + 0.007*\"chines\"')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model_bow.print_topics(num_topics = 5, num_words = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Save the model to local path\n",
    "'''\n",
    "path = 'model/lda_bow.pkl'\n",
    "\n",
    "# save the classifier\n",
    "with open(path, 'wb') as fn:\n",
    "    cPickle.dump(lda_model_bow, fn)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4': LDA Model with TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Train TF-IDF (TfidfModel) model and fit into documents (tfidf). TF-IDF is for term frequency - inverse document frequency. Calculation details: multiplying a local component (term frequency) with a global component (inverse document frequency), and normalizing the resulting documents to unit length.\n",
    "\n",
    "2. Train LDA model with tfidf (LdaMulticore). Note that this uses the same LDA model as previous, but different data inputs.\n",
    "3. Print out the topics.\n",
    "4. Save model to path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel, LdaMulticore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create TF-IDF model for each document i.e realize the transformation between word-document co-occurrence matrix (int) \n",
    "into a locally/globally weighted TF-IDF matrix (positive floats). \n",
    "\n",
    "Save the global model to 'tfidf' and the realization to corpus_tfidf.\n",
    "'''\n",
    "# fit model\n",
    "tfidf = TfidfModel(bow_corpus)\n",
    "# apply model to the corpus documents\n",
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Train your lda model using gensim.models.LdaMulticore and save it to 'lda_model'\n",
    "'''\n",
    "# TODO\n",
    "lda_model_tfidf =  LdaMulticore(corpus_tfidf, \n",
    "                                   num_topics = 10, \n",
    "                                   id2word = dictionary,                                    \n",
    "                                   passes = 10,\n",
    "                                   workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5,\n",
       "  '0.015*\"weight\" + 0.010*\"gene\" + 0.010*\"command\" + 0.010*\"needl\" + 0.008*\"diet\"'),\n",
       " (4,\n",
       "  '0.036*\"gordon\" + 0.014*\"chastiti\" + 0.014*\"cadr\" + 0.014*\"surrend\" + 0.014*\"intellect\"'),\n",
       " (3,\n",
       "  '0.004*\"diseas\" + 0.003*\"patient\" + 0.003*\"mail\" + 0.003*\"satellit\" + 0.003*\"project\"'),\n",
       " (0,\n",
       "  '0.015*\"alaska\" + 0.012*\"dyer\" + 0.008*\"aurora\" + 0.008*\"nsmca\" + 0.007*\"spdcc\"'),\n",
       " (7,\n",
       "  '0.016*\"vnet\" + 0.014*\"food\" + 0.009*\"smoke\" + 0.008*\"jupit\" + 0.008*\"portal\"')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model_tfidf.print_topics(num_topics = 5, num_words = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Save the model to local path\n",
    "'''\n",
    "path = 'model/lda_tfidf.pkl'\n",
    "\n",
    "# save the classifier\n",
    "with open(path, 'wb') as fn:\n",
    "    cPickle.dump(lda_model_tfidf, fn)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Apply the Model on Training Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If interested in the performance of training data. Otherwise, skip this part.\n",
    "1. Load the model (cPickle.load)\n",
    "2. Apply the model to document (model_result)\n",
    "\n",
    "    a. transform the document into bow (doc2bow)\n",
    "    \n",
    "    b. apply the model to data (model[bow_vector])\n",
    "    \n",
    "    c. print the topics and possibilities (print_topic) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load the models\n",
    "'''  \n",
    "path = '/Users/stephanie/Job_Market/github/DS/LDA/model/lda_bow.pkl'\n",
    "with open(path, 'rb') as fn:\n",
    "    lda_model_bow = cPickle.load(fn)    \n",
    "\n",
    "path = '/Users/stephanie/Job_Market/github/DS/LDA/model/lda_tfidf.pkl'\n",
    "with open(path, 'rb') as fn:\n",
    "    lda_model_tfidf = cPickle.load(fn)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_result(model, document):\n",
    "    bow_vector = dictionary.doc2bow(preprocess(document))\n",
    "    \n",
    "    for index, score in sorted(model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "        print(\"Score: {}\\t Topic: {}\".format(score, model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_num = 10  # any num within [0, 1186] -- training data\n",
    "document = newsgroups_train.data[document_num]\n",
    "#print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.979994535446167\t Topic: 0.015*\"mission\" + 0.011*\"satellit\" + 0.010*\"spacecraft\" + 0.009*\"shuttl\" + 0.009*\"mar\"\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Result of LDA Using Bag of Words\n",
    "'''\n",
    "model_result(lda_model_bow, document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.5161755681037903\t Topic: 0.010*\"stage\" + 0.009*\"centaur\" + 0.008*\"mission\" + 0.007*\"cancer\" + 0.007*\"appl\"\n",
      "Score: 0.3118633031845093\t Topic: 0.023*\"henri\" + 0.019*\"toronto\" + 0.014*\"larc\" + 0.010*\"spencer\" + 0.009*\"zoolog\"\n",
      "Score: 0.1282937079668045\t Topic: 0.004*\"diseas\" + 0.003*\"patient\" + 0.003*\"mail\" + 0.003*\"satellit\" + 0.003*\"project\"\n",
      "Score: 0.030328504741191864\t Topic: 0.022*\"digex\" + 0.013*\"express\" + 0.012*\"onlin\" + 0.011*\"communic\" + 0.008*\"carl\"\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Result of LDA Using TF-IDF\n",
    "'''\n",
    "model_result(lda_model_tfidf, document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Apply the Model to Test Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If interested in the performance of test data. Otherwise, skip this part.\n",
    "1. Load the model (cPickle.load)\n",
    "2. Apply the model to document (model_result)\n",
    "\n",
    "    a. transform the document into bow (doc2bow)\n",
    "    \n",
    "    b. apply the model to data (model[bow_vector])\n",
    "    \n",
    "    c. print the topics and possibilities (print_topic) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load the models\n",
    "'''  \n",
    "path = '/Users/stephanie/Job_Market/github/DS/LDA/model/lda_bow.pkl'\n",
    "with open(path, 'rb') as fn:\n",
    "    lda_model_bow = cPickle.load(fn)    \n",
    "\n",
    "path = '/Users/stephanie/Job_Market/github/DS/LDA/model/lda_tfidf.pkl'\n",
    "with open(path, 'rb') as fn:\n",
    "    lda_model_tfidf = cPickle.load(fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_result(model, document):\n",
    "    bow_vector = dictionary.doc2bow(preprocess(document))\n",
    "    \n",
    "    for index, score in sorted(model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "        print(\"Score: {}\\t Topic: {}\".format(score, model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_num = 100 # any num within [0, 789] -- test data\n",
    "unseen_document = newsgroups_test.data[document_num]\n",
    "#print(unseen_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.5368219614028931\t Topic: 0.015*\"mission\" + 0.011*\"satellit\" + 0.010*\"spacecraft\" + 0.009*\"shuttl\" + 0.009*\"mar\"\n",
      "Score: 0.3186612129211426\t Topic: 0.014*\"satellit\" + 0.013*\"fund\" + 0.010*\"commerci\" + 0.010*\"market\" + 0.010*\"project\"\n",
      "Score: 0.11858393996953964\t Topic: 0.016*\"patient\" + 0.016*\"pain\" + 0.015*\"diseas\" + 0.010*\"candida\" + 0.008*\"infect\"\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Result of LDA Using Bag of Words\n",
    "'''\n",
    "model_result(lda_model_bow, unseen_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.7678722143173218\t Topic: 0.004*\"diseas\" + 0.003*\"patient\" + 0.003*\"mail\" + 0.003*\"satellit\" + 0.003*\"project\"\n",
      "Score: 0.11889021098613739\t Topic: 0.015*\"alaska\" + 0.012*\"dyer\" + 0.008*\"aurora\" + 0.008*\"nsmca\" + 0.007*\"spdcc\"\n",
      "Score: 0.08730076253414154\t Topic: 0.010*\"stage\" + 0.009*\"centaur\" + 0.008*\"mission\" + 0.007*\"cancer\" + 0.007*\"appl\"\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Result of LDA Using TF-IDF\n",
    "'''\n",
    "model_result(lda_model_tfidf, unseen_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Extra: BOW or TF-IDF? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. BOW: summarize the term frequency.\n",
    "2. TF-IDF: summarize the term frequency and inverse document frequency. bow_corpus is the input of TF-IDF as well.\n",
    "\n",
    "For feeding LDA, a paper by Blei and Lafferty (2009) suggests that TF-IDF is not necessry but could be helpful. \n",
    "\n",
    "paper: http://www.cs.columbia.edu/~blei/papers/BleiLafferty2009.pdf\n",
    "\n",
    "Stackflow discussion: https://stackoverflow.com/questions/44781047/necessary-to-apply-tf-idf-to-new-documents-in-gensim-lda-model/44789327#44789327"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 9 (\"human\") appears 1 time, with tf-idf as 0.11119258084160516.\n",
      "Word 39 (\"soon\") appears 1 time, with tf-idf as 0.1013654407969559.\n",
      "Word 70 (\"consid\") appears 1 time, with tf-idf as 0.10688973455146679.\n",
      "Word 79 (\"school\") appears 1 time, with tf-idf as 0.11655844119462777.\n",
      "Word 96 (\"mayb\") appears 2 time, with tf-idf as 0.21022179855651255.\n",
      "Word 102 (\"shuttl\") appears 1 time, with tf-idf as 0.10874765465476678.\n",
      "Word 123 (\"current\") appears 1 time, with tf-idf as 0.10176540863607408.\n",
      "Word 183 (\"worth\") appears 1 time, with tf-idf as 0.1417022709780529.\n",
      "Word 204 (\"offic\") appears 1 time, with tf-idf as 0.11598977500433545.\n",
      "Word 217 (\"administr\") appears 1 time, with tf-idf as 0.14384614193604373.\n",
      "Word 238 (\"mar\") appears 3 time, with tf-idf as 0.42510681293415875.\n",
      "Word 254 (\"daniel\") appears 1 time, with tf-idf as 0.1461059528303259.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Preview BOW & TF-IDF for our sample preprocessed document\n",
    "'''\n",
    "document_num = 20\n",
    "bow_doc_x = bow_corpus[document_num]\n",
    "corpus_tfidf_x = corpus_tfidf[document_num]\n",
    "#print corpus_tfidf_x[1][0]\n",
    "\n",
    "for i in range(len(bow_doc_x)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time, with tf-idf as {}.\".format(bow_doc_x[i][0], \n",
    "                                                     dictionary[bow_doc_x[i][0]], \n",
    "                                                     bow_doc_x[i][1],\n",
    "                                                     corpus_tfidf_x[i][1]))\n",
    "    if i > 10:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
