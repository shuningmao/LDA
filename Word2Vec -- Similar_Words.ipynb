{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reference:\n",
    "\n",
    "original paper: https://arxiv.org/abs/1301.3781\n",
    "\n",
    "gensim package manual: https://radimrehurek.com/gensim/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "choose data from sklearn.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "logging.basicConfig() # to show logging message of what is happening behind the scene.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sci.med', 'sci.space']\n",
      "(1187,) (1187,)\n",
      "(790,) (790,)\n"
     ]
    }
   ],
   "source": [
    "categories = ['sci.med', 'sci.space']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', shuffle = True, categories = categories)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', shuffle = True, categories = categories)\n",
    "\n",
    "print(list(newsgroups_train.target_names))\n",
    "\n",
    "print(newsgroups_train.filenames.shape, newsgroups_train.target.shape)\n",
    "print(newsgroups_test.filenames.shape, newsgroups_test.target.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Pre-processing  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the document (simple_preprocess). \n",
    "\n",
    "E.g. sentence: \"This is a sentence for illustration of the data pre-processing.\"\n",
    "\n",
    "    After tokenize: ['this', 'is', 'sentence', 'for', 'illustration', 'of', 'the', 'data', 'pre', 'processing']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Loading Gensim and nltk libraries\n",
    "'''\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Write a function to perform the pre processing steps on the entire dataset\n",
    "'''\n",
    "# Tokenize\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in simple_preprocess(text) :\n",
    "        result.append(token)\n",
    "            \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = []\n",
    "\n",
    "for doc in newsgroups_train.data:\n",
    "    processed_docs.append(preprocess(doc))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Build a Word2Vec Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a Word2Vec model, which produce word embeddings in multiple dimensions (Word2Vec)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(processed_docs, \n",
    "                 size=200, window=5, min_count=5, \n",
    "                 alpha=0.02, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Find Similar Words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the similar words (most_similar). Based on Word2Vec model, each word can be compared with other words in terms of closeness in hundreds of dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isbn 0.9995366334915161\n",
      "increases 0.9995232820510864\n",
      "south 0.9995055198669434\n",
      "databases 0.9994800090789795\n",
      "promote 0.9994581937789917\n",
      "ad 0.9994315505027771\n",
      "chemistry 0.9994108080863953\n",
      "pm 0.9994007349014282\n",
      "array 0.9993683099746704\n",
      "standards 0.999338686466217\n",
      "astronomical 0.99931401014328\n",
      "aiaa 0.9992887377738953\n",
      "west 0.9992662668228149\n",
      "interstellar 0.9992375373840332\n",
      "review 0.999211311340332\n",
      "optical 0.9992091059684753\n",
      "pp 0.9992085099220276\n",
      "ref 0.9991738200187683\n",
      "aerobee 0.9991406202316284\n",
      "cdc 0.9990933537483215\n",
      "outbreak 0.9990758895874023\n",
      "staff 0.9990637302398682\n",
      "newsgroups 0.9990483522415161\n",
      "aerospace 0.9990434646606445\n",
      "cell 0.9990332126617432\n"
     ]
    }
   ],
   "source": [
    "interest_word = \"physics\"\n",
    "try:\n",
    "    similar_words = model.wv.most_similar(interest_word, topn = 25)\n",
    "\n",
    "    for (word, similarity) in similar_words:\n",
    "        print(word, similarity)\n",
    "except:\n",
    "    print(\"The word: {} is not in the dictionary.\".format(interest_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
